<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Thousand TCP flow with iperf - Ferenc Fejes technical blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="Ferenc Fejes"><meta name=description content="How to do correct iperf measurements with many TCP flows on GNU/Linux test environment"><meta name=keywords content="Ferenc,Fejes,MPTCP,proxy,eBPF,linux,networking"><meta name=generator content="Hugo 0.85.0 with theme even"><link rel=canonical href=/net/2019/07/14/many-tcp-flow-with-iperf/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.b5a744db6de49a86cadafb3b70f555ab443f83c307a483402259e94726b045ff.css rel=stylesheet><link href=/lib/fancybox/jquery.fancybox-3.1.20.min.css rel=stylesheet><meta property="og:title" content="Thousand TCP flow with iperf"><meta property="og:description" content="How to do correct iperf measurements with many TCP flows on GNU/Linux test environment"><meta property="og:type" content="article"><meta property="og:url" content="/net/2019/07/14/many-tcp-flow-with-iperf/"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-07-14T15:00:00+00:00"><meta property="article:modified_time" content="2021-07-17T20:03:24+02:00"><meta itemprop=name content="Thousand TCP flow with iperf"><meta itemprop=description content="How to do correct iperf measurements with many TCP flows on GNU/Linux test environment"><meta itemprop=datePublished content="2019-07-14T15:00:00+00:00"><meta itemprop=dateModified content="2021-07-17T20:03:24+02:00"><meta itemprop=wordCount content="929"><meta itemprop=keywords content="tcp,measurement,iperf,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Thousand TCP flow with iperf"><meta name=twitter:description content="How to do correct iperf measurements with many TCP flows on GNU/Linux test environment"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Ferenc Fejes technical blog</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Ferenc Fejes technical blog</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Thousand TCP flow with iperf</h1><div class=post-meta><span class=post-time>2019-07-14</span><div class=post-category><a href=/categories/net/>net</a></div><span class=more-meta>929 words</span>
<span class=more-meta>5 mins read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents></nav></div></div><div class=post-content><h1 id=abstract>Abstract</h1><p>The classic <strong>iperf</strong> and lately the <strong>iperf3</strong> widely used for various network performance measurements. The original iperf written in C++ while the iperf3 written in C, but thats not the only difference, from performance measurement standpoint there is a big one: iperf3 is single threaded and iperf is multi-threaded. This is quite important when it comes to multiple flow TCP measurements. In many cases, iperf3 fails to utilize the whole network capacity due to CPU bottleneck: each TCP flow share one thread and therefore one CPU core. In contrast, iperf create a new thread for every TCP flow.
Nevertheless handling many TCP flows is still tricky even with the multi-threaded iperf. In this blogpost, we will investigate the pitfalls of many flow measurements.</p><h1 id=the-naive-approach>The naive approach</h1><p>First, we have to install iperf. In my Ubuntu 18.04 environment, this looks like a following.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>$ sudo apt install iperf
</code></pre></td></tr></table></div></div><p>To verify the operation, we can test the TCP performance on loopback. We need to start an iperf server and client in separated terminal sessions:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>$ iperf -s #server
</code></pre></td></tr></table></div></div><p>Then the client:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>$ iperf -c 0
</code></pre></td></tr></table></div></div><p>I got the following output from the client in my notebook:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>spyff@hp:~$ iperf -c 0
------------------------------------------------------------
Client connecting to 0, TCP port 5001
TCP window size: 2.50 MByte (default)
------------------------------------------------------------
[  3] local 127.0.0.1 port 40232 connected with 127.0.0.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.0 sec  45.1 GBytes  38.8 Gbits/sec
</code></pre></td></tr></table></div></div><p>That was a single TCP flow measurement. We can repeat the test with 2 flows, just pass the <code>-P 2</code> parameter to the client iperf:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>spyff@hp:~$ iperf -c 0 -P 2
------------------------------------------------------------
Client connecting to 0, TCP port 5001
TCP window size: 2.50 MByte (default)
------------------------------------------------------------
[  4] local 127.0.0.1 port 40258 connected with 127.0.0.1 port 5001
[  3] local 127.0.0.1 port 40256 connected with 127.0.0.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0-10.0 sec  33.6 GBytes  28.8 Gbits/sec
[  3]  0.0-10.0 sec  34.1 GBytes  29.3 Gbits/sec
[SUM]  0.0-10.0 sec  67.7 GBytes  58.1 Gbits/sec
</code></pre></td></tr></table></div></div><h1 id=the-problem-of-small-backlog>The problem of small backlog</h1><p>Unfortunately, this is not a very scalable approach, if we try it with 1000 flows it will fail. In my case, the iperf trying to spawn threads but that happens very slowly and some flow even fails with the following error: <code>write failed: Connection reset by peer</code>. Unfortunately that&rsquo;s not a very helpful error message, but it means that the remote peer got too many incoming connections which fills his listener TCP socket&rsquo;s backlog queue full. When the backlog is full, the kernel refuses to accept new incoming TCP connections so send TCP reset back to the iperf client. But at the same time, the backlog also processed by the kernel, so some new TCP flow will succeed to connect because of the liberated places in the backlog queue. To check how big the listener&rsquo;s backlog, we can use the <code>perf trace</code> utility. The <code>perf trace</code> can monitor the system calls used by iperf, so we have to monitor the <code>listen</code> system call of the iperf server, and check the length of the backlog:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>spyff@hp:~$ sudo perf trace -e listen
     0.000 ( 0.025 ms): iperf/13104 listen(fd: 3&lt;socket:[379613]&gt;, backlog: 2147483647)  = 0
</code></pre></td></tr></table></div></div><p>In my case the backlog is big enough (<code>MAX_INT</code>), because I use the iperf version <code>2.0.10</code>. But in older version, the backlog was set to <code>5</code> which is too small for many connections. So there is another limit somewhere which prevent us from our successful measurements: the <code>SOMAXCONN</code> kernel parameter. This is set to <code>128</code> by default. We can increase it to <code>65536</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>spyff@hp:~$ sysctl net.core.somaxconn
net.core.somaxconn = 128
spyff@hp:~$ 
spyff@hp:~$ sudo sysctl -w net.core.somaxconn=65536
net.core.somaxconn = 65536
</code></pre></td></tr></table></div></div><p>Now the <code>write failed: Connection reset by peer</code> error should be gone.</p><h1 id=iperf-fails-to-report-the-aggregated-throughput>iperf fails to report the aggregated throughput</h1><p>With <code>-P 100</code> we can see the aggregeated throughput in the last row <code>[SUM]</code> at the end of the measurement:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-fallback data-lang=fallback>...
[ 44]  0.0-10.2 sec   726 MBytes   595 Mbits/sec
[ 43]  0.0-10.1 sec   621 MBytes   516 Mbits/sec
[ 47]  0.0-10.1 sec   597 MBytes   495 Mbits/sec
[ 32]  0.0-10.1 sec   767 MBytes   636 Mbits/sec
[ 58]  0.0-10.2 sec   333 MBytes   273 Mbits/sec
[SUM]  0.0-10.2 sec  49.2 GBytes  41.3 Gbits/sec

</code></pre></td></tr></table></div></div><p>However, this row disappearing at <code>-P 1000</code> or to be more precise, at <code>-P 128</code>. This is really suspicious for anyone who coded in C/C++ and experienced integer overflow with signed <code>char</code> type. I investigated the place of the problem in the iperf source code, and it turned out somewhere in the aggregating code it compares a <code>char</code> with an <code>int</code> and increases both at every iteration (for every thread). Then the <code>char</code> overflows, and the condition <code>if(char == int)</code> will obviously evaluate as <code>false</code>. I submitted the bug to the maintainer of <code>iperf</code>, you can find the commit <a href=https://sourceforge.net/p/iperf2/code/ci/be417e3be47fa4929c06cf4080b756fbc270d0f1/>here</a>. I recommend using the latest version of iperf to avoid that bug.</p><h1 id=summary>Summary</h1><ul><li>iperf is better for multi TCP flow measurement than iperf3, because multi-threaded</li><li>Older versions of iperf explicitly set backlog length to <code>5</code> which is insufficient, so use iperf version <code>>= 2.0.10</code> or modify the value in the iperf source code manually to a greater value</li><li><code>sudo sysctl -w net.core.somaxconn=65536</code>: in linux, set <code>SOMAXCONN</code> value from the default <code>128</code> to <code>65536</code></li><li>If the aggregated throughput <code>[SUM]</code> disappears with big parallel-flow values (<code>-P > 127</code>) use the latest version of iperf: <a href=https://sourceforge.net/p/iperf2/code/ci/master/tree/>iperf master branch at Sourceforge</a></li></ul></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Ferenc Fejes</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2021-07-17
<a href=https://github.com/SPYFF/spyff.github.io//commit/2e0ecd6f0199a0811178175d698deb34d70d9477 title="update theme">(2e0ecd6)</a></span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/ target=_blank>CC BY-NC-ND 4.0</a></span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/tcp/>tcp</a>
<a href=/tags/measurement/>measurement</a>
<a href=/tags/iperf/>iperf</a></div><nav class=post-nav><a class=next href=/linux/2018/07/20/qemu-vm/><span class="next-text nav-default">QEMU virtual machine for kernel testing</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:primalgamer@gmail.com class="iconfont icon-email" title=email></a><a href=https://twitter.com/spyff0 class="iconfont icon-twitter" title=twitter></a><a href=https://www.linkedin.com/in/fejes/ class="iconfont icon-linkedin" title=linkedin></a><a href=https://github.com/SPYFF class="iconfont icon-github" title=github></a><a href=/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a></span>
<span class=copyright-year>&copy;
2017 -
2021<span class=heart><i class="iconfont icon-heart"></i></span><span>Ferenc Fejes</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/lib/fancybox/jquery.fancybox-3.1.20.min.js></script><script type=text/javascript src=/js/main.min.c99b103c33d1539acf3025e1913697534542c4a5aa5af0ccc20475ed2863603b.js></script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-104300360-1','auto'),ga('set','anonymizeIp',!0),ga('send','pageview'))</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>